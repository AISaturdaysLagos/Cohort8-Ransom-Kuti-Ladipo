{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from torchprofile import profile_macs\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline Transformer Module\n",
    "\n",
    "This module contains the implementation of a Transformer model for sign language tasks.\n",
    "\n",
    "Classes:\n",
    "- TokenEmbedding: Create embedding for the target seqeunce\n",
    "- LandmarkEmbedding: Create embedding for the source(frames)seqeunce\n",
    "- Encoder: Implements the transformer encoder stack.\n",
    "- Decoder: Implements the transformer decoder stack.\n",
    "- Transformer: The main transformer model class with methods for training and inference.\n",
    "\n",
    "Methods:\n",
    "- Transformer.generate: Perform inference on a new sequence\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Embed the tokens with postion encoding\"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, maxlen, embedding_dim):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_vocab : int\n",
    "            number of vocabulary\n",
    "        maxlen : int\n",
    "            maximuin length of sequence\n",
    "        embedding_dim : int\n",
    "            embedding output dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embed_layer = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.postion_embed_layer = nn.Embedding(maxlen, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tensors\n",
    "            _description_\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensors\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        maxlen = x.size(-1)\n",
    "        x = self.token_embed_layer(x)\n",
    "        positions = torch.arange(0, maxlen).to(x.device)\n",
    "        positions = self.postion_embed_layer(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    \"\"\"_summary_\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Calculate the padding for \"same\" padding\n",
    "        padding = (11 - 1) // 2\n",
    "\n",
    "        # Define three 1D convolutional layers with ReLU activation and stride 2\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=1, out_channels=64, kernel_size=11, stride=2, padding=padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=64, out_channels=128, kernel_size=11, stride=2, padding=padding\n",
    "        )\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=128, out_channels=256, kernel_size=11, stride=2, padding=padding\n",
    "        )\n",
    "\n",
    "        # Output embedding layer\n",
    "        self.embedding_layer = nn.Linear(256, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x should have shape (batch_size, input_size, input_dim)\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension for 1D convolution\n",
    "\n",
    "        # Apply convolutional layers with ReLU activation and stride 2\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "\n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        # Apply the linear embedding layer\n",
    "        x = self.embedding_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"_summary_\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_heads,\n",
    "        feed_forward_dim,\n",
    "        rate=0.1,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embedding_dim : _type_\n",
    "            _description_\n",
    "        num_heads : _type_\n",
    "            _description_\n",
    "        feed_forward_dim : _type_\n",
    "            _description_\n",
    "        rate : float, optional\n",
    "            _description_, by default 0.1\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.multi_attention = nn.MultiheadAttention(embedding_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, feed_forward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feed_forward_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, inputs_x):\n",
    "        multi_attention_out, _ = self.multi_attention(inputs_x, inputs_x, inputs_x)\n",
    "        multi_attention_out = self.dropout1(multi_attention_out)\n",
    "        out1 = self.layernorm1(inputs_x + multi_attention_out)\n",
    "\n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        x = self.layernorm2(out1 + ffn_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"_summary_\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads_ = num_heads\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.layernorm3 = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.decoder_multi_attention = nn.MultiheadAttention(embedding_dim, num_heads)\n",
    "        self.encoder_multi_attention = nn.MultiheadAttention(embedding_dim, num_heads)\n",
    "        self.decoder_dropout = nn.Dropout(0.5)\n",
    "        self.encoder_dropout = nn.Dropout(dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, feed_forward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feed_forward_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def _causal_attention_mask(self, sequence_length, batch_size=1, device=None):\n",
    "        mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1).to(\n",
    "            device\n",
    "        )\n",
    "        mask = mask.unsqueeze(0).expand(\n",
    "            batch_size * self.num_heads_, sequence_length, sequence_length\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        encoder_out,\n",
    "        src_target_,\n",
    "    ):\n",
    "        input_shape = src_target_.size()\n",
    "        batch_size = 1  # input_shape[0]\n",
    "        seq_len = input_shape[0]\n",
    "        x_device = src_target_.device\n",
    "\n",
    "        # Mask\n",
    "        causal_mask = self._causal_attention_mask(\n",
    "            sequence_length=seq_len, batch_size=batch_size, device=x_device\n",
    "        )\n",
    "\n",
    "        target_att, _ = self.decoder_multi_attention(\n",
    "            src_target_, src_target_, src_target_, attn_mask=causal_mask\n",
    "        )\n",
    "        target_norm_out = self.layernorm1(\n",
    "            src_target_ + self.decoder_dropout(target_att)\n",
    "        )\n",
    "\n",
    "        encoder_out, _ = self.encoder_multi_attention(\n",
    "            target_norm_out, encoder_out, encoder_out\n",
    "        )\n",
    "        enc_out_norm = self.layernorm2(encoder_out + self.encoder_dropout(encoder_out))\n",
    "\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n",
    "\n",
    "\n",
    "class ASLTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden_dim=64,\n",
    "        multi_num_head=8,\n",
    "        num_feed_forward=128,\n",
    "        target_maxlen=64,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=4,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_hidden_dim : int, optional\n",
    "            _description_, by default 64\n",
    "        multi_num_head : int, optional\n",
    "            _description_, by default 8\n",
    "        num_feed_forward : int, optional\n",
    "            _description_, by default 128\n",
    "        target_maxlen : int, optional\n",
    "            _description_, by default 64\n",
    "        num_layers_enc : int, optional\n",
    "            _description_, by default 4\n",
    "        num_layers_dec : int, optional\n",
    "            _description_, by default 4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = 62\n",
    "\n",
    "        self.encoder_input = LandmarkEmbedding(embedding_dim=num_hidden_dim)\n",
    "        self.decoder_input = TokenEmbedding(\n",
    "            num_vocab=self.num_classes,\n",
    "            embedding_dim=num_hidden_dim,\n",
    "            maxlen=target_maxlen,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.encoder_input,\n",
    "            *[\n",
    "                TransformerEncoder(\n",
    "                    embedding_dim=num_hidden_dim,\n",
    "                    num_heads=multi_num_head,\n",
    "                    feed_forward_dim=num_feed_forward,\n",
    "                )\n",
    "                for _ in range(num_layers_enc)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            self.add_module(\n",
    "                f\"decoder_layer_{i}\",\n",
    "                TransformerDecoder(\n",
    "                    embedding_dim=num_hidden_dim,\n",
    "                    num_heads=multi_num_head,\n",
    "                    feed_forward_dim=num_feed_forward,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=num_hidden_dim, out_features=self.num_classes\n",
    "        )\n",
    "\n",
    "    def _decoder_run(self, enc_out, target):\n",
    "        decoder_out = self.decoder_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            decoder_out = getattr(self, f\"decoder_layer_{i}\")(enc_out, decoder_out)\n",
    "        return decoder_out\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if len(source.shape) == 2:  # Check if single input\n",
    "            source = source.unsqueeze(0)  # Add batch dimension\n",
    "        if len(target.shape) == 1:  # Check if single input\n",
    "            target = target.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        encoder_out = self.encoder(source)\n",
    "        transformer_output = self._decoder_run(encoder_out, target)\n",
    "        return self.classifier(transformer_output)\n",
    "\n",
    "    def generate(self, source, target_start_token_idx=60):\n",
    "        if len(source.shape) == 2:  # Check if single input\n",
    "            source = source.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        encoder_out = self.encoder(source)\n",
    "        decoder_input = (\n",
    "            torch.ones((source.shape[0], 1), dtype=torch.long)\n",
    "            .to(source.device)\n",
    "            * target_start_token_idx\n",
    "        )\n",
    "        dec_logits = []\n",
    "\n",
    "        for _ in range(self.target_maxlen - 1):\n",
    "            decoder_out = self._decoder_run(encoder_out, decoder_input)\n",
    "            logits = self.classifier(decoder_out)\n",
    "\n",
    "            logits = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            last_logit = logits[:, -1]\n",
    "            dec_logits.append(last_logit)\n",
    "            decoder_input = torch.cat([decoder_input, last_logit], dim=-1)\n",
    "\n",
    "        return decoder_input.squeeze(0) if len(source.shape) == 2 else decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-11 03:21:41.460\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1m ERROR Message ==> Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 128, 345]\u001b[0m\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "    │   └ <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    └ <module 'ipykernel.kernelapp' from 'c:\\\\Users\\\\Yinka\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\ipyker...\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1041, in launch_instance\n",
      "    app.start()\n",
      "    │   └ <function IPKernelApp.start at 0x0000017524A5C400>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x000001751F4770D0>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 724, in start\n",
      "    self.io_loop.start()\n",
      "    │    │       └ <function BaseAsyncIOLoop.start at 0x0000017524A5D3A0>\n",
      "    │    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x0000017524AAA910>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x000001751F4770D0>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    │    │            └ <function BaseEventLoop.run_forever at 0x000001752137EAC0>\n",
      "    │    └ <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x0000017524AAA910>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "    │    └ <function BaseEventLoop._run_once at 0x0000017521380900>\n",
      "    └ <_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "    │      └ <function Handle._run at 0x00000175213309A0>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...510>, ...],))>)>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    │    │            │    │           │    └ <member '_args' of 'Handle' objects>\n",
      "    │    │            │    │           └ <Handle Task.task_wakeup(<Future finis...510>, ...],))>)>\n",
      "    │    │            │    └ <member '_callback' of 'Handle' objects>\n",
      "    │    │            └ <Handle Task.task_wakeup(<Future finis...510>, ...],))>)>\n",
      "    │    └ <member '_context' of 'Handle' objects>\n",
      "    └ <Handle Task.task_wakeup(<Future finis...510>, ...],))>)>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 512, in dispatch_queue\n",
      "    await self.process_one()\n",
      "          │    └ <function Kernel.process_one at 0x0000017523406C00>\n",
      "          └ <ipykernel.ipkernel.IPythonKernel object at 0x0000017524A71F90>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 501, in process_one\n",
      "    await dispatch(*args)\n",
      "          │         └ ([<zmq.sugar.frame.Frame object at 0x0000017524A3E150>, <zmq.sugar.frame.Frame object at 0x0000017524A3E210>, <zmq.sugar.fram...\n",
      "          └ <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object at 0x0000017524A71F90>>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 408, in dispatch_shell\n",
      "    await result\n",
      "          └ <coroutine object Kernel.execute_request at 0x000001753359F060>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 731, in execute_request\n",
      "    reply_content = await reply_content\n",
      "                          └ <coroutine object IPythonKernel.do_execute at 0x000001753357FDC0>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 417, in do_execute\n",
      "    res = shell.run_cell(\n",
      "          │     └ <function ZMQInteractiveShell.run_cell at 0x0000017524A49760>\n",
      "          └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000017524AC5090>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "                             │       └ {'store_history': True, 'silent': False, 'cell_id': 'vscode-notebook-cell:/c%3A/Main%20Workspace/Cohort8-Ransom-Kuti-Ladipo/l...\n",
      "                             └ ('# Create a sample input\\nbatch_source_sequence = torch.randn(2, 128, 345)  # Sample source sequence (batch_size, maxlen, nu...\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2945, in run_cell\n",
      "    result = self._run_cell(\n",
      "             │    └ <function InteractiveShell._run_cell at 0x0000017522A823E0>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000017524AC5090>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3000, in _run_cell\n",
      "    return runner(coro)\n",
      "           │      └ <coroutine object InteractiveShell.run_cell_async at 0x0000017524B85080>\n",
      "           └ <function _pseudo_sync_runner at 0x0000017522A75620>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "    │    └ <method 'send' of 'coroutine' objects>\n",
      "    └ <coroutine object InteractiveShell.run_cell_async at 0x0000017524B85080>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3203, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "                       │    │             │        │     └ 'C:\\\\Users\\\\Yinka\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_3812\\\\4012752373.py'\n",
      "                       │    │             │        └ [<ast.Assign object at 0x00000175339678B0>, <ast.Assign object at 0x0000017533967E80>, <ast.Assign object at 0x0000017533967E...\n",
      "                       │    │             └ <ast.Module object at 0x0000017533967B50>\n",
      "                       │    └ <function InteractiveShell.run_ast_nodes at 0x0000017522A82700>\n",
      "                       └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000017524AC5090>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3382, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "             │    │        │     │              └ False\n",
      "             │    │        │     └ <ExecutionResult object at 1753225ad90, execution_count=7 error_before_exec=None error_in_exec=None info=<ExecutionInfo objec...\n",
      "             │    │        └ <code object <module> at 0x0000017533D041B0, file \"C:\\Users\\Yinka\\AppData\\Local\\Temp\\ipykernel_3812\\4012752373.py\", line 1>\n",
      "             │    └ <function InteractiveShell.run_code at 0x0000017522A827A0>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000017524AC5090>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "         │         │    │               │    └ {'__name__': '__main__', '__doc__': '\\nBaseline Transformer Module\\n\\nThis module contains the implementation of a Transforme...\n",
      "         │         │    │               └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000017524AC5090>\n",
      "         │         │    └ <property object at 0x0000017522A6E160>\n",
      "         │         └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000017524AC5090>\n",
      "         └ <code object <module> at 0x0000017533D041B0, file \"C:\\Users\\Yinka\\AppData\\Local\\Temp\\ipykernel_3812\\4012752373.py\", line 1>\n",
      "\n",
      "> File \"\u001b[32mC:\\Users\\Yinka\\AppData\\Local\\Temp\\ipykernel_3812\\\u001b[0m\u001b[32m\u001b[1m4012752373.py\u001b[0m\", line \u001b[33m18\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mpredictions\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mtransformer_model\u001b[0m\u001b[1m(\u001b[0m\u001b[1msingle_src_seq\u001b[0m\u001b[1m,\u001b[0m \u001b[1msingle_trg_seq\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m              │                 │               └ \u001b[0m\u001b[36m\u001b[1mtensor([53,  3, 20, 29, 50, 25, 29, 13, 15, 32, 10, 15, 32, 36, 23, 50, 53,  5,\u001b[0m\n",
      "    \u001b[36m              │                 │                 \u001b[0m\u001b[36m\u001b[1m        43, 37, 41, 58, 30, 12,  6, 40, 31, 4...\u001b[0m\n",
      "    \u001b[36m              │                 └ \u001b[0m\u001b[36m\u001b[1mtensor([[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\u001b[0m\n",
      "    \u001b[36m              │                   \u001b[0m\u001b[36m\u001b[1m        [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0.4944...\u001b[0m\n",
      "    \u001b[36m              └ \u001b[0m\u001b[36m\u001b[1mASLTransformer(\u001b[0m\n",
      "    \u001b[36m                \u001b[0m\u001b[36m\u001b[1m  (encoder_input): LandmarkEmbedding(\u001b[0m\n",
      "    \u001b[36m                \u001b[0m\u001b[36m\u001b[1m    (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,)...\u001b[0m\n",
      "\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           │             │       └ {}\n",
      "           │             └ (tensor([[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "           │                       [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0.494...\n",
      "           └ <bound method ASLTransformer.forward of ASLTransformer(\n",
      "               (encoder_input): LandmarkEmbedding(\n",
      "                 (conv1): Conv1d(1, 64, kern...\n",
      "\n",
      "  File \"\u001b[32mC:\\Users\\Yinka\\AppData\\Local\\Temp\\ipykernel_3812\\\u001b[0m\u001b[32m\u001b[1m440851353.py\u001b[0m\", line \u001b[33m285\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    \u001b[1mencoder_out\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mencoder\u001b[0m\u001b[1m(\u001b[0m\u001b[1msource\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m              │            └ \u001b[0m\u001b[36m\u001b[1mtensor([[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\u001b[0m\n",
      "    \u001b[36m              │              \u001b[0m\u001b[36m\u001b[1m         [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0.49...\u001b[0m\n",
      "    \u001b[36m              └ \u001b[0m\u001b[36m\u001b[1mASLTransformer(\u001b[0m\n",
      "    \u001b[36m                \u001b[0m\u001b[36m\u001b[1m  (encoder_input): LandmarkEmbedding(\u001b[0m\n",
      "    \u001b[36m                \u001b[0m\u001b[36m\u001b[1m    (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,)...\u001b[0m\n",
      "\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           │             │       └ {}\n",
      "           │             └ (tensor([[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "           │                        [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0.4...\n",
      "           └ <bound method Sequential.forward of Sequential(\n",
      "               (0): LandmarkEmbedding(\n",
      "                 (conv1): Conv1d(1, 64, kernel_size=(11,), strid...\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            │      └ tensor([[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "            │                 [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0.49...\n",
      "            └ LandmarkEmbedding(\n",
      "                (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))\n",
      "                (conv2): Conv1d(64, 128, kernel_s...\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           │             │       └ {}\n",
      "           │             └ (tensor([[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "           │                        [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0.4...\n",
      "           └ <bound method LandmarkEmbedding.forward of LandmarkEmbedding(\n",
      "               (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(2,), paddin...\n",
      "\n",
      "  File \"\u001b[32mC:\\Users\\Yinka\\AppData\\Local\\Temp\\ipykernel_3812\\\u001b[0m\u001b[32m\u001b[1m440851353.py\u001b[0m\", line \u001b[33m86\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    \u001b[1mx\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mtorch\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mrelu\u001b[0m\u001b[1m(\u001b[0m\u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mconv1\u001b[0m\u001b[1m(\u001b[0m\u001b[1mx\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m    │     │    │          └ \u001b[0m\u001b[36m\u001b[1mtensor([[[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\u001b[0m\n",
      "    \u001b[36m    │     │    │            \u001b[0m\u001b[36m\u001b[1m          [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0....\u001b[0m\n",
      "    \u001b[36m    │     │    └ \u001b[0m\u001b[36m\u001b[1mLandmarkEmbedding(\u001b[0m\n",
      "    \u001b[36m    │     │      \u001b[0m\u001b[36m\u001b[1m  (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))\u001b[0m\n",
      "    \u001b[36m    │     │      \u001b[0m\u001b[36m\u001b[1m  (conv2): Conv1d(64, 128, kernel_s...\u001b[0m\n",
      "    \u001b[36m    │     └ \u001b[0m\u001b[36m\u001b[1m<built-in method relu of type object at 0x00007FFEA23CC560>\u001b[0m\n",
      "    \u001b[36m    └ \u001b[0m\u001b[36m\u001b[1m<module 'torch' from 'c:\\\\Users\\\\Yinka\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\torch\\\\__init__.py'>\u001b[0m\n",
      "\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           │             │       └ {}\n",
      "           │             └ (tensor([[[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "           │                         [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0...\n",
      "           └ <bound method Conv1d.forward of Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))>\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 313, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           │    │             │      │            └ Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))\n",
      "           │    │             │      └ Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))\n",
      "           │    │             └ tensor([[[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "           │    │                         [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0....\n",
      "           │    └ <function Conv1d._conv_forward at 0x000001752F154180>\n",
      "           └ Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))\n",
      "  File \"c:\\Users\\Yinka\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 309, in _conv_forward\n",
      "    return F.conv1d(input, weight, bias, self.stride,\n",
      "           │ │      │      │       │     │    └ (2,)\n",
      "           │ │      │      │       │     └ Conv1d(1, 64, kernel_size=(11,), stride=(2,), padding=(5,))\n",
      "           │ │      │      │       └ Parameter containing:\n",
      "           │ │      │      │         tensor([-0.1697, -0.2340, -0.0869,  0.0877, -0.2328,  0.2527, -0.2036,  0.0177,\n",
      "           │ │      │      │                 -0.2673, -0.069...\n",
      "           │ │      │      └ Parameter containing:\n",
      "           │ │      │        tensor([[[-0.1422, -0.0277,  0.0092, -0.0871,  0.2008, -0.2052, -0.2548,\n",
      "           │ │      │                   0.0125,  0.0531,  0...\n",
      "           │ │      └ tensor([[[[0.4061, 0.6164, 0.6337,  ..., 0.5066, 0.4483, 0.4846],\n",
      "           │ │                  [0.0074, 0.1099, 0.6021,  ..., 0.5536, 0.7102, 0....\n",
      "           │ └ <built-in method conv1d of type object at 0x00007FFEA23CC560>\n",
      "           └ <module 'torch.nn.functional' from 'c:\\\\Users\\\\Yinka\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\torch\\...\n",
      "\n",
      "\u001b[31m\u001b[1mRuntimeError\u001b[0m:\u001b[1m Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 128, 345]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input\n",
    "batch_source_sequence = torch.randn(2, 128, 345)  # Sample source sequence (batch_size, maxlen, num_hid)\n",
    "batch_target_sequence = torch.randint(0, 60, (2, 64))  # Sample target sequence (batch_size, maxlen)\n",
    "single_src_seq = torch.rand(128,345)\n",
    "single_trg_seq = torch.randint(0,60,(64,))\n",
    "\n",
    "try:\n",
    "    # Instantiate the Transformer model\n",
    "    transformer_model = ASLTransformer(\n",
    "        num_hidden_dim=200,\n",
    "        multi_num_head= 4,\n",
    "        num_feed_forward=400,\n",
    "        target_maxlen=64,\n",
    "        num_layers_enc=2,\n",
    "        num_layers_dec=1,)\n",
    "\n",
    "    # Forward pass to get predictions\n",
    "    predictions = transformer_model(single_src_seq, single_trg_seq)\n",
    "\n",
    "    # Print the shape of the predictions\n",
    "    print(f\"final {predictions.shape}\")\n",
    "    \n",
    "except Exception as error:\n",
    "    logger.exception(f\" ERROR Message ==> {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 346])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your original sequence tensor\n",
    "original_tensor = torch.randn(118, 346)\n",
    "\n",
    "# Define the desired output shape\n",
    "desired_shape = (128, 346)\n",
    "\n",
    "# Calculate the padding on the first dimension from the bottom\n",
    "padding_bottom = max(0, desired_shape[0] - original_tensor.size(0))\n",
    "\n",
    "# Pad the tensor along the first dimension from the bottom\n",
    "padded_tensor = torch.nn.functional.pad(original_tensor, (0, 0, 0, padding_bottom))\n",
    "\n",
    "# Now, padded_tensor has the shape (128, 346)\n",
    "print(padded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0490,  1.5782, -0.0793, -0.8889, -0.6999],\n",
       "        [ 0.3881,  1.1002, -0.7594, -1.0423,  1.1450],\n",
       "        [ 2.1911,  0.6852,  0.7096, -1.1343, -0.3205]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tensor[115:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0490,  1.5782, -0.0793, -0.8889, -0.6999],\n",
       "        [ 0.3881,  1.1002, -0.7594, -1.0423,  1.1450],\n",
       "        [ 2.1911,  0.6852,  0.7096, -1.1343, -0.3205],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tensor[115:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
